{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fc4c57-4dbf-4626-85af-084aeda3478c",
   "metadata": {},
   "source": [
    "#  Custom Models and Training with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a90e35-a250-4315-82ee-94d9dcaa4bcb",
   "metadata": {},
   "source": [
    "## Using TensorFlow like NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a279f-4e33-4ea6-80c4-50673c51771d",
   "metadata": {},
   "source": [
    " Tensor and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e81758-bcd3-471f-8747-acdeda6de119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4ddac3-4217-4cb8-a78e-1a2c2f26476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67883e8b-fae9-405f-a2c1-48050d542ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9d939-3675-41a8-957a-2d3cf175b90a",
   "metadata": {},
   "source": [
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbf9a27-34dd-4b46-9209-1a09391a8085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45435a8d-e3af-4064-beda-32523d3400dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f00b4cc-b7a6-45cc-a37f-035bd295e7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a4d4e-12ca-4c15-bc24-d5571496b3b2",
   "metadata": {},
   "source": [
    "Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009766fb-8f15-48fa-b81b-7b78cff0a329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0b6d8a-67b0-4496-a08b-7b8088ce0ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c316f06b-36af-46ac-84da-ffe9fff09e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103085f6-4271-4c2e-bfb1-e5c529855c1b",
   "metadata": {},
   "source": [
    "Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03adc8bb-cd7d-40ec-9c6d-672c5d843bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d7142-3a8d-4a45-8ebe-a45fc7d95cfc",
   "metadata": {},
   "source": [
    "### Tensors and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6a49f-dd7a-4a87-93d2-a2f558691b64",
   "metadata": {},
   "source": [
    "`TensorFlow uses float32 by default and NumPy uses float64 by default. Remember to adjust dtype before conversion from numpy to tensorflow.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee3a276-561f-4ac9-b411-b810973e4227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a37920-9e2c-40aa-a355-2e0ffe787821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy() # or np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b640950-c3ad-4fc4-b80f-bd80caa2ad9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb98329-b334-4d82-bd4b-7dc77f972e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d025b8-cc80-4c51-b363-e9a32e8558c6",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d041900-4282-4786-afc9-86214d6f1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "155953db-009d-473a-a557-1912ae4301fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81974031-c465-46d0-b31d-c486661bbb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc4857-c96a-43fd-878c-c9570dc2be57",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37123fe-20be-4c1b-91ca-d44dd3e9136f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92038ba5-cfcb-4c72-9375-2427834ed4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce979471-0e87-426f-b5c0-5bbb439afbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7dfbe3-dfe6-4427-b908-7061620b5a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd79586d-081b-4840-9a65-848eb94d3198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(\n",
    "    indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4157a812-19dd-4abe-b730-84b0f40904af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "# Direct assignment does not work\n",
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6c60f-b658-4aca-81bd-bd8c40c6ed58",
   "metadata": {},
   "source": [
    "## Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf07b70-e692-401f-bf23-864106db1757",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eacbe97-78a0-4e93-bb42-ab86bac8001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred_):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91af323b-23a2-4805-80bf-cd369658b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this Huber loss function when you compile the keras model\n",
    "# model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "# model.fit(X_train, y_train, [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ac994-2179-462c-83c8-33468e827cce",
   "metadata": {},
   "source": [
    "### Saving Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66a03830-3a51-45c6-af25-1d0b6a418cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"my_model_with_a_custom_loss\",\n",
    "                                   # custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444c6c6-455c-4239-a849-c94e4431dbb1",
   "metadata": {},
   "source": [
    "Do not need to include it in custom_objects if huber_fn() function is decorated with @keras.utils.register_keras_serializable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e840619-6de6-42c8-b4b2-8719bafc7818",
   "metadata": {},
   "source": [
    "Create a function that creates a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a292e72-7527-4171-860f-19f99fb63f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "# model.compile(loss=create_huber(2.0), optimzer=\"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d91e86-93e3-4c67-b852-6eae6105840f",
   "metadata": {},
   "source": [
    "When you save the model, the threshold will not be saved. Have to specify threshold value when loading hte model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30afe8fa-bd8e-407b-819b-30e302ae7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\n",
    "#     \"my_model_with_a_custom_loss_threshold_2\",\n",
    "#     custom_objects={\"huber_fn\": create_huber(2.0)}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919813de-f3ce-4667-ba25-d16c21fd8a9c",
   "metadata": {},
   "source": [
    "Can solve this by creating a subclass of the tf.keras.losses.Loss class, and then implementing its get_config() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "909a47dc-eadc-4fe5-8758-e1b94d0a51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super.__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super.get_config()\n",
    "        return {**base_config, \"threshold\" : self.threshold} # can do base_config | {\"threshold\" : self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fe649-209d-43d8-a112-66cc87f9726f",
   "metadata": {},
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b294c9d3-87e0-4e23-a007-2f058e60f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return values is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6a91567-d708-42c4-8ad3-a3d95678845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments depend on the type of custom function. These custom functions can then be used normally, as shown here:\n",
    "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                              kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer,\n",
    "                              kernel_constraint=my_positive_weights\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99340a6d-9a7e-4c73-8e21-2d426810c903",
   "metadata": {},
   "source": [
    "If function has hyperparameters that need to be saved along with the model, then subclass appropriate class such as: tf.keras.regularizers.Regularizer, tf.keras.constraints.Constraint, tf.keras.initializers.Initializer, or tf.keras.layers.Layer (for any layer, including activation functions). Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95644f28-4927-4c1c-8f12-1f061fc460fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\" : self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7094df-60c9-4834-8376-7d4e90b19e80",
   "metadata": {},
   "source": [
    "Note that you must implement the call() method for losses, layers (including activation functions), and models, or the \\_\\_call__() method for regularizer, initializers, and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c39c4-1f03-4114-ba9f-7d460e792201",
   "metadata": {},
   "source": [
    "### Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62989a5d-024c-4884-ba26-30544e9ce53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaf9c4db-4a23-4eb4-8af2-6e2cd00a6a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.800000011920929>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tf.keras.metrics.Precision() # streaming metric (or stateful metric)\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddcd84e5-aba4-4276-b746-3f30e237a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70d3cee5-bb57-44de-95a0-dd7b0d3f2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d67a20cf-70bd-410a-a1d8-00888bb67b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=precision/true_positives, shape=(1,), dtype=float32, value=[4.]>,\n",
       " <Variable path=precision/false_positives, shape=(1,), dtype=float32, value=[4.]>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "383f5a62-6a21-451e-acb4-7ec42274491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_state() # both variables get reset to 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e803f3-ccc6-400a-83de-20d152ae89ed",
   "metadata": {},
   "source": [
    "If you need to define your own custom streaming metric, create a subclass of the tf.keras.metrics.Metric class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83b449d2-01f7-4484-8d12-95ebae469232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for illustration purposes\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\" : self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bf35587-b291-4401-9fac-8694fc1e74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better implementation\n",
    "class HuberMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name=\"HuberMetric\", dtype=None):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super().update_state(metric, sample_weight)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329751e-f247-4aed-ae4b-322f6f197400",
   "metadata": {},
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f39bfd8c-3bba-4897-b319-afc0b6657824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create custom layer without any weights, the simplest option is to write a function and wrap it in a tf.keras.layers.Lambda layer\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.expo(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fe65795-237b-427d-8cf1-5c78646ae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a custom stateful layer (i.e, a layer with weights)\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activation.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\" : self.units,\n",
    "                \"activation\" : tf.keras.activations.serialize(self.activation)}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "027026ec-16a5-49f1-81f8-bf9993f564fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer with multiple inputs\n",
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2,  X1 * X2, X1 / X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a312ab18-6ee1-4bbf-9ecd-bac297ffbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer that has a different behavior during training\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=False):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), sddev=self.stddev)\n",
    "            return X + noise\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a21d07-91ab-4e8c-a0ce-9db8047aa805",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afadbf50-ca82-4d19-8389-d12b3d7088f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create this residual layer\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n",
    "                                            kernel_initializer=\"he_normal\")\n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ead55f2-decc-479e-9c37-00500aefc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using subclassing API to define model\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                            kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cb8e9-f140-4bc4-b3c6-67171c9a06b4",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9ec1a87-8615-43ac-97da-633f5b7e9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for this custom model with a custom reconstruction loss and a corresponding metric\n",
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, ouput_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layer.Dense(30, activation=\"relu\",\n",
    "                                            kernel_initializer=\"he_normal\")\n",
    "                        for _ in range(5)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return sel.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c5e69-5237-49f0-9b0e-c58a9714f177",
   "metadata": {},
   "source": [
    "### Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25f1e406-0c7c-4dea-8603-82e06bb87f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c41ca04-20f3-43e0-88a9-df96982070e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91f32672-3b9f-420e-b3bc-f19bca99151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0354f83a-671e-4fe9-af52-ed991b32e397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using reverse-mode autodiff.\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994c845-359e-4c28-8da6-338db9134667",
   "metadata": {},
   "source": [
    "In order to save memory, only put the strict minimum inside the tf.GradientTape() block. Alternatively, pause recording by creating a with tape.stop_recording() block inside the tf.GradientTape() block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c7d65b9-400f-4194-a33b-b097674ecde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "# tape is automatically erased after calling its gradient() once:\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1) # returns tensor 36.0\n",
    "\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2) # raises a RuntimeError!\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c74097d5-5b56-4082-acff-ec1006814c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to call gradient() more than once, you must make the tape persistent and delete it each time you are done with it to free resources\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3673f2d5-8150-4e2e-9f25-aa9a678631ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tape only keeps track of variables so if use constant tensors:\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd61d460-d18a-49cd-88e3-dc8ec5c90f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you may ocasionally run into some numerical issues when computing gradients\n",
    "x = tf.Variable(1e-50)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8856fff-55ea-48cc-904b-dad64ef8828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better implementation of softplus\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd9a7363-3932-4948-b126-99f567dd2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in some rare cases, a numerically stable function may still have numerically unstable gradients. Have to tell TensorFlow which equation to use for the gradients\n",
    "@tf.custom_gradient\n",
    "def my_softplus(z):\n",
    "    def my_softplus_gradients(grads): # grads = backprop'ed from upper layers\n",
    "        return grads * (1 - 1 / (1 + tf.exp(z))) # stable grads of softplus\n",
    "\n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "    return result, my_softplus_gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05424485-fcc9-48e8-8021-384722daff21",
   "metadata": {},
   "source": [
    "### Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3cdf302-e963-4483-aa36-0fcc4896b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "160b9fe9-280e-473a-8732-06a36cefa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple model. There is not need to compile it\n",
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2_reg),\n",
    "    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "555525b6-4e73-42ea-8f29-0f232fd62eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tiny function that will randomly sample a batch of instances from the training set\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4463b77-6fe7-4107-8c60-2d34afbdaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
    "                          for m in [loss] + (metrics or[])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75b22287-0f13-4ec4-874b-dd7b455d1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e3d9bb5-c5ac-4717-98e3-e7bc20d02b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "362/362 - mean_loss: 0.3882 - mean_absolute_error: 0.3808\n",
      "Epoch 2/5\n",
      "362/362 - mean_loss: 0.5888 - mean_absolute_error: 0.5676\n",
      "Epoch 3/5\n",
      "362/362 - mean_loss: 0.6170 - mean_absolute_error: 0.5002\n",
      "Epoch 4/5\n",
      "362/362 - mean_loss: 0.6372 - mean_absolute_error: 0.4944\n",
      "Epoch 5/5\n",
      "362/362 - mean_loss: 0.4913 - mean_absolute_error: 0.4201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75c55ded-3469-46a9-8e51-a727930e408b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e62a7321e349f18d9ef47b6409e7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23f01eb5c2e488fbf2d6bfad2615158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a4719c6a1b4c7dbac6959d1f098855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dbb376d7074fe2856f5c0c03d4bb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c4cc86f514fdcba6c1f1a084d97b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec4121dc11c4675865bec7dac2a08aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pasted code from Author's notebook. Shows how to use the tqdm package to display nice progress bars\n",
    "# extra code â€“ shows how to use the tqdm package to display nice progress bars\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "\n",
    "                steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71394506-f2a6-4603-859a-ca1c4633274b",
   "metadata": {},
   "source": [
    "## TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0362e5ea-2d60-4ecb-be14-5a4799dd7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30862324-4455-4fc5-83d7-5819b2672ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9884c7a-51f0-4bab-a019-18c21bb62a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x189a2c67ed0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a346ce9e-80fd-41ee-90ac-432e96858271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65aa3b36-e1c7-4bb5-9ffd-ac48de793879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a063e65c-146e-4d9f-b933-bb5c3f22cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "677e9ec5-061f-4ce1-86a4-1c945d231e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e2e3f-5fdd-4469-9a16-9019679af176",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75f7c8-ff57-4984-a5b7-65bee1b94246",
   "metadata": {},
   "source": [
    "1. a library for computational graphs. Efficient computation and ease of use. Jax and Pytorch.\n",
    "2. No, TensorFlow is not a replacement for NumPy. The main difference is the TensorFlow main data structure is tensors and, for NumPy, is the arrays. TensorFlow also tries to perform computations efficiently automatically with whatever function we create.\n",
    "3. Yes, as NumPy and TensorFlow make conversion of tensors and arrays very easy.\n",
    "4. Queue, sparse tensors, ragged tensors, tensor arrays, sets\n",
    "5. Function is for when we do not have a hyperparameter, subclassing is for when we want to save the hyperparameter when saving the model\n",
    "6. Same answer as question 5's\n",
    "7. Create a custom layer when we can do what we want in a single layer. Create a custom model when we want different layers to interact in a different way.\n",
    "8. More control over the training loop, debugging, training in a different way than provided by TensorFlow.\n",
    "9. Must be convertible to TF functions.\n",
    "10. only use tf operations, if creating variables or datasets do so in the first line of code or outside function, do not use other libraries and not even pythond standard libraries.\n",
    "11. When we want to use functions from other libraries. When compiling model, set eager execution to True. If all models are dynamic, they could become much less efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dc435-d2ed-459a-bcf2-c07bdfe833f6",
   "metadata": {},
   "source": [
    "12. Implement a custom layer that performs layer normalization:\n",
    "\n",
    "    a. The build() method should define two trainable weights $\\alpha$ and $\\beta$, both of shape input_shape [-1:] and data type tf.float32. $\\alpha$ should be initialized with 1s, and $\\beta$ with 0s\n",
    "\n",
    "    b. The call() method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean $\\mu$ and the variance $\\sigma^2$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return *Î±*âŠ—(*X* - Î¼)/(Ïƒ + Îµ) + *Î²*, where âŠ— represents itemwise multiplication (`*`) and Îµ is a smoothing term (small constant to avoid division by zero, e.g., 0.001).\n",
    "\n",
    "    c. _Ensure that your custom layer produces the same (or very nearly the same) output as the `tf.keras.layers.LayerNormalization` layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b65803e-bae4-4b7b-bef0-d9d9fd9c36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=input_shape[-1:],\n",
    "            initializer=\"zeros\")\n",
    "\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance) + 0.0001) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "855c87f1-5e3f-41bf-b20c-a275a0baca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "81855daf-e52c-4712-b801-ddb3e3cc828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the California housing dataset\n",
    "model_tf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=X_train.shape[-1:]),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "my_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=X_train.shape[-1:]),\n",
    "    Layer_norm(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dcb6f963-1ff9-42dd-8aa2-99563a66d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\"RootMeanSquaredError\"],\n",
    "    optimizer=\"SGD\")\n",
    "\n",
    "my_model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\"RootMeanSquaredError\"],\n",
    "    optimizer=\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7619e065-fc6b-4e20-a32c-ab631b5015e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.2124 - loss: 1.4700 - val_RootMeanSquaredError: 1.1429 - val_loss: 1.3063\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1557 - loss: 1.3356 - val_RootMeanSquaredError: 1.1873 - val_loss: 1.4097\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1547 - loss: 1.3332 - val_RootMeanSquaredError: 1.1422 - val_loss: 1.3047\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1518 - loss: 1.3266 - val_RootMeanSquaredError: 1.1618 - val_loss: 1.3499\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1499 - loss: 1.3224 - val_RootMeanSquaredError: 1.1349 - val_loss: 1.2880\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1459 - loss: 1.3132 - val_RootMeanSquaredError: 1.2064 - val_loss: 1.4553\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1466 - loss: 1.3147 - val_RootMeanSquaredError: 1.1246 - val_loss: 1.2647\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1430 - loss: 1.3064 - val_RootMeanSquaredError: 1.1209 - val_loss: 1.2563\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1412 - loss: 1.3022 - val_RootMeanSquaredError: 1.1183 - val_loss: 1.2507\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1407 - loss: 1.3011 - val_RootMeanSquaredError: 1.1805 - val_loss: 1.3937\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1361 - loss: 1.2906 - val_RootMeanSquaredError: 1.1359 - val_loss: 1.2904\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1362 - loss: 1.2910 - val_RootMeanSquaredError: 1.1104 - val_loss: 1.2329\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1365 - loss: 1.2916 - val_RootMeanSquaredError: 1.1112 - val_loss: 1.2347\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1291 - loss: 1.2748 - val_RootMeanSquaredError: 1.1215 - val_loss: 1.2578\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1296 - loss: 1.2759 - val_RootMeanSquaredError: 1.2327 - val_loss: 1.5195\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1346 - loss: 1.2873 - val_RootMeanSquaredError: 1.1688 - val_loss: 1.3660\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1250 - loss: 1.2656 - val_RootMeanSquaredError: 1.1826 - val_loss: 1.3986\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1276 - loss: 1.2714 - val_RootMeanSquaredError: 1.0981 - val_loss: 1.2059\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1142 - loss: 1.2415 - val_RootMeanSquaredError: 1.1689 - val_loss: 1.3664\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1148 - loss: 1.2428 - val_RootMeanSquaredError: 1.2088 - val_loss: 1.4611\n"
     ]
    }
   ],
   "source": [
    "history_tf = model_tf.fit(X_train, y_train, \n",
    "                       epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1a8d2228-36f3-45f8-a439-03453be43701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1869 - loss: 1.4087 - val_RootMeanSquaredError: 1.1514 - val_loss: 1.3257\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1652 - loss: 1.3578 - val_RootMeanSquaredError: 1.1407 - val_loss: 1.3013\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1594 - loss: 1.3443 - val_RootMeanSquaredError: 1.1454 - val_loss: 1.3120\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1577 - loss: 1.3402 - val_RootMeanSquaredError: 1.1386 - val_loss: 1.2965\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1537 - loss: 1.3310 - val_RootMeanSquaredError: 1.1859 - val_loss: 1.4064\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1523 - loss: 1.3278 - val_RootMeanSquaredError: 1.1322 - val_loss: 1.2820\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1501 - loss: 1.3227 - val_RootMeanSquaredError: 1.1312 - val_loss: 1.2796\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1482 - loss: 1.3185 - val_RootMeanSquaredError: 1.1288 - val_loss: 1.2742\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1469 - loss: 1.3154 - val_RootMeanSquaredError: 1.1250 - val_loss: 1.2657\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1451 - loss: 1.3112 - val_RootMeanSquaredError: 1.1245 - val_loss: 1.2645\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1430 - loss: 1.3065 - val_RootMeanSquaredError: 1.1206 - val_loss: 1.2557\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1376 - loss: 1.2941 - val_RootMeanSquaredError: 1.1589 - val_loss: 1.3429\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1406 - loss: 1.3011 - val_RootMeanSquaredError: 1.1425 - val_loss: 1.3054\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1367 - loss: 1.2922 - val_RootMeanSquaredError: 1.1148 - val_loss: 1.2428\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1304 - loss: 1.2777 - val_RootMeanSquaredError: 1.1092 - val_loss: 1.2302\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1329 - loss: 1.2834 - val_RootMeanSquaredError: 1.1028 - val_loss: 1.2162\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1305 - loss: 1.2780 - val_RootMeanSquaredError: 1.1295 - val_loss: 1.2757\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1296 - loss: 1.2759 - val_RootMeanSquaredError: 1.1588 - val_loss: 1.3429\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1271 - loss: 1.2702 - val_RootMeanSquaredError: 1.0947 - val_loss: 1.1984\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - RootMeanSquaredError: 1.1206 - loss: 1.2558 - val_RootMeanSquaredError: 1.0887 - val_loss: 1.1853\n"
     ]
    }
   ],
   "source": [
    "my_history = my_model.fit(X_train, y_train, \n",
    "                       epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "31eef410-f72e-4539-8356-d2f0954c8f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[-0.29537582, -0.16629328, -0.24113165, -0.2219448 ,  0.11663638,\n",
       "        -0.11351932, -0.4765796 , -0.23885104],\n",
       "       [-0.32881823, -0.21005034, -0.25207087, -0.2365596 ,  0.11811031,\n",
       "        -0.1262686 , -0.58191866, -0.11767381],\n",
       "       [-0.30603322, -0.13173658, -0.23529656, -0.22523968,  0.11615811,\n",
       "        -0.11617221, -0.46847165, -0.2633689 ],\n",
       "       [-0.251659  , -0.15667798, -0.22586706, -0.21513076,  0.11538444,\n",
       "        -0.10295809, -0.43709666, -0.3030616 ]], dtype=float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.layers[0](X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e5536f0-d746-4842-a91b-0788f49b5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[-0.18491364, -0.18655874, -0.15223306, -0.06520963,  0.57598996,\n",
       "        -0.22528985, -0.43214753, -0.30542195],\n",
       "       [-0.21388632, -0.23079644, -0.16345519, -0.07829991,  0.5789715 ,\n",
       "        -0.24051759, -0.52364355, -0.18486273],\n",
       "       [-0.19414663, -0.15162255, -0.14624715, -0.06816083,  0.5750226 ,\n",
       "        -0.22845843, -0.42510512, -0.32981485],\n",
       "       [-0.14703983, -0.17683779, -0.13657376, -0.05910635,  0.5734575 ,\n",
       "        -0.21267551, -0.39785323, -0.369305  ]], dtype=float32)>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.layers[0](X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1eb631ef-3715-495d-b656-87275c07ed0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[-0.11046219,  0.02026546, -0.08889858, -0.15673517, -0.45935357,\n",
       "         0.11177053, -0.04443207,  0.06657091],\n",
       "       [-0.11493191,  0.0207461 , -0.08861569, -0.15825969, -0.4608612 ,\n",
       "         0.11424899, -0.0582751 ,  0.06718893],\n",
       "       [-0.11188659,  0.01988597, -0.08904941, -0.15707885, -0.45886445,\n",
       "         0.11228622, -0.04336652,  0.06644595],\n",
       "       [-0.10461918,  0.02015981, -0.0892933 , -0.15602441, -0.45807302,\n",
       "         0.10971742, -0.03924343,  0.06624341]], dtype=float32)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf.layers[0](X_train[:4]) - my_model.layers[0](X_train[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04705fca-95aa-491a-84f9-eb0bb7c36c07",
   "metadata": {},
   "source": [
    "The difference looks pretty small to me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c75fd4-7497-43a5-8bce-fa25a84b470e",
   "metadata": {},
   "source": [
    "## 13. Train a model using a custom training loop to tackle the Fashion MNIST dataset\n",
    "_The Fashion MNIST dataset was introduced in Chapter 10._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1eb93-c2aa-4753-b8fc-603562894178",
   "metadata": {},
   "source": [
    "### a.\n",
    "_Exercise: Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch._\n",
    "\n",
    "### b.\n",
    "_Exercise: Try using a different optimizer with a different learning rate for the upper layers and the lower layers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e9fd7c7d-58a2-466a-84b9-ae55108ac947",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "645efd36-ee2d-4ceb-9411-cac71167a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "38c6338d-9803-490d-8493-ba71cb7ede15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    indexes = np.random.default_rng().integers(X.shape[0], size=batch_size)\n",
    "\n",
    "    return X[indexes], y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8cf3e233-f2b3-4fe0-90d5-63277de4b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(batch, batches, loss, metric):\n",
    "    print(f\"\\rBatch {batch}/{batches} - loss: {loss.result()} - Accuracy: {metric.result()} \", end=\"\")\n",
    "\n",
    "def print_val(loss, metric):\n",
    "        print(f\"- val_loss: {loss} - val_accuracy: {metric.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e0423e4c-c3b2-4f97-8dad-25da60f4d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=[28, 28]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2f777d84-1b33-49fe-a8cc-8a0e64ccd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "optimizer_upper = tf.keras.optimizers.SGD(1e-4)\n",
    "optimizer_lower = tf.keras.optimizers.Adam(1e-3)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metric = tf.keras.metrics.Accuracy()\n",
    "num_variables = len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "94c95090-4aa3-4bfa-b060-932aaafaf9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch 1719/1719 - loss: 0.5173409581184387 - Accuracy: 0.8196625709533691 - val_loss: 0.43423324823379517 - val_accuracy: 0.843999981880188\n",
      "Epoch 2/10\n",
      "Batch 1719/1719 - loss: 0.4438561499118805 - Accuracy: 0.8689281344413757 - val_loss: 0.3812102973461151 - val_accuracy: 0.8611999750137329\n",
      "Epoch 3/10\n",
      "Batch 1719/1719 - loss: 0.40463095903396606 - Accuracy: 0.884289562702179 - val_loss: 0.4023357033729553 - val_accuracy: 0.8529999852180481\n",
      "Epoch 4/10\n",
      "Batch 1719/1719 - loss: 0.38221275806427 - Accuracy: 0.8871254920959473 - val_loss: 0.37169259786605835 - val_accuracy: 0.8669999837875366\n",
      "Epoch 5/10\n",
      "Batch 1719/1719 - loss: 0.3636491894721985 - Accuracy: 0.8953606486320496 - val_loss: 0.357223778963089 - val_accuracy: 0.8700000047683716\n",
      "Epoch 6/10\n",
      "Batch 1719/1719 - loss: 0.34774789214134216 - Accuracy: 0.9021960496902466 - val_loss: 0.3755651116371155 - val_accuracy: 0.8715999722480774\n",
      "Epoch 7/10\n",
      "Batch 1719/1719 - loss: 0.3351919949054718 - Accuracy: 0.9061227440834045 - val_loss: 0.3639211654663086 - val_accuracy: 0.870199978351593\n",
      "Epoch 8/10\n",
      "Batch 1719/1719 - loss: 0.323731005191803 - Accuracy: 0.9114128947257996 - val_loss: 0.36836498975753784 - val_accuracy: 0.8755999803543091\n",
      "Epoch 9/10\n",
      "Batch 1719/1719 - loss: 0.31377696990966797 - Accuracy: 0.9139761328697205 - val_loss: 0.37276020646095276 - val_accuracy: 0.8705999851226807\n",
      "Epoch 10/10\n",
      "Batch 1719/1719 - loss: 0.3054049611091614 - Accuracy: 0.9160849452018738 - val_loss: 0.35408374667167664 - val_accuracy: 0.8766000270843506\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    \n",
    "    for batch in range(1, batches + 1):\n",
    "        X_batch, y_true = get_batch(X_train, y_train, batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_true, y_pred))\n",
    "            loss = tf.reduce_sum([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "        optimizer_lower.apply_gradients(zip(gradients[:num_variables//2], model.trainable_variables[:num_variables//2]))\n",
    "        optimizer_upper.apply_gradients(zip(gradients[num_variables//2:], model.trainable_variables[num_variables//2:]))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        accuracy = metric(y_true, tf.argmax(y_pred, axis=1))\n",
    "        print_status(batch, batches, mean_loss, metric)\n",
    "        \n",
    "    metric.reset_state()\n",
    "    y_pred_valid = model(X_valid)\n",
    "    val_loss = tf.reduce_mean(loss_fn(y_valid, y_pred_valid))\n",
    "    metric(y_valid, tf.argmax(y_pred_valid, axis=1))\n",
    "    print_val(val_loss, metric)\n",
    "    metric.reset_state()\n",
    "    mean_loss.reset_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d254f-90fd-4ec9-9323-6ebb047babaa",
   "metadata": {},
   "source": [
    "### Author's solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86751901-c209-41ea-a510-ba8a0f1eee86",
   "metadata": {},
   "source": [
    "1. TensorFlow is an open-source library for numerical computation, particularly well suited and fine-tuned for large-scale Machine Learning. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis and optimization capabilities (with a portable graph format that allows you to train a TensorFlow model in one environment and run it in another), an optimization API based on reverse-mode autodiff, and several powerful APIs such as tf.keras, tf.data, tf.image, tf.signal, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer.\n",
    "2. Although TensorFlow offers most of the functionalities provided by NumPy, it is not a drop-in replacement, for a few reasons. First, the names of the functions are not always the same (for example, `tf.reduce_sum()` versus `np.sum()`). Second, some functions do not behave in exactly the same way (for example, `tf.transpose()` creates a transposed copy of a tensor, while NumPy's `T` attribute creates a transposed view, without actually copying any data). Lastly, NumPy arrays are mutable, while TensorFlow tensors are not (but you can use a `tf.Variable` if you need a mutable object).\n",
    "3. Both `tf.range(10)` and `tf.constant(np.arange(10))` return a one-dimensional tensor containing the integers 0 to 9. However, the former uses 32-bit integers while the latter uses 64-bit integers. Indeed, TensorFlow defaults to 32 bits, while NumPy defaults to 64 bits.\n",
    "4. Beyond regular tensors, TensorFlow offers several other data structures, including sparse tensors, tensor arrays, ragged tensors, queues, string tensors, and sets. The last two are actually represented as regular tensors, but TensorFlow provides special functions to manipulate them (in `tf.strings` and `tf.sets`).\n",
    "5. When you want to define a custom loss function, in general you can just implement it as a regular Python function. However, if your custom loss function must support some hyperparameters (or any other state), then you should subclass the `keras.losses.Loss` class and implement the `__init__()` and `call()` methods. If you want the loss function's hyperparameters to be saved along with the model, then you must also implement the `get_config()` method.\n",
    "6. Much like custom loss functions, most metrics can be defined as regular Python functions. But if you want your custom metric to support some hyperparameters (or any other state), then you should subclass the `keras.metrics.Metric` class. Moreover, if computing the metric over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch (e.g., as for the precision and recall metrics), then you should subclass the `keras.metrics.Metric` class and implement the `__init__()`, `update_state()`, and `result()` methods to keep track of a running metric during each epoch. You should also implement the `reset_state()` method unless all it needs to do is reset all variables to 0.0. If you want the state to be saved along with the model, then you should implement the `get_config()` method as well.\n",
    "7. You should distinguish the internal components of your model (i.e., layers or reusable blocks of layers) from the model itself (i.e., the object you will train). The former should subclass the `keras.layers.Layer` class, while the latter should subclass the `keras.models.Model` class.\n",
    "8. Writing your own custom training loop is fairly advanced, so you should only do it if you really need to. Keras provides several tools to customize training without having to write a custom training loop: callbacks, custom regularizers, custom constraints, custom losses, and so on. You should use these instead of writing a custom training loop whenever possible: writing a custom training loop is more error-prone, and it will be harder to reuse the custom code you write. However, in some cases writing a custom training loop is necessaryâ â€”for example, if you want to use different optimizers for different parts of your neural network, like in the [Wide & Deep paper](https://homl.info/widedeep). A custom training loop can also be useful when debugging, or when trying to understand exactly how training works.\n",
    "9. Custom Keras components should be convertible to TF Functions, which means they should stick to TF operations as much as possible and respect all the rules listed in Chapter 12 (in the _TF Function Rules_ section). If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a `tf.py_function()` operation (but this will reduce performance and limit your model's portability) or set `dynamic=True` when creating the custom layer or model (or set `run_eagerly=True` when calling the model's `compile()` method).\n",
    "10. Please refer to Chapter 12 for the list of rules to respect when creating a TF Function (in the _TF Function Rules_ section).\n",
    "11. Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set `dynamic=True` when creating it. Alternatively, you can set `run_eagerly=True` when calling the model's `compile()` method. Making a model dynamic prevents Keras from using any of TensorFlow's graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model's portability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3cce4-9be6-49ef-b2f4-2c77ed2a1ce2",
   "metadata": {},
   "source": [
    "### 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d192e142-451d-44eb-9c52-7d40681925d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"zeros\")\n",
    "\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"eps\": self.eps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2e13c-6f0a-464c-b43c-04b6776aeea3",
   "metadata": {},
   "source": [
    "Note that making _Îµ_ a hyperparameter (`eps`) was not compulsory. Also note that it's preferable to compute `tf.sqrt(variance + self.eps)` rather than `tf.sqrt(variance) + self.eps`. Indeed, the derivative of sqrt(z) is undefined when z=0, so training will bomb whenever the variance vector has at least one component equal to 0. Adding _Îµ_ within the square root guarantees that this will never happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd586c2a-22d5-4049-9567-65aa0315473c",
   "metadata": {},
   "source": [
    "Let's create one instance of each class, apply them to some data (e.g., the training set), and ensure that the difference is negligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cb5c7c96-5a89-496f-999c-cd0e7c02d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.682965837332631e-08>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = LayerNormalization()\n",
    "keras_layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e7a63-d40e-4523-8170-e5850a2dbfe1",
   "metadata": {},
   "source": [
    "Yep, that's close enough. To be extra sure, let's make alpha and beta completely random and compare again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "193632fc-38de-4974-9547-d85065891497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.438514279390347e-08>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "random_alpha = np.random.rand(X.shape[-1])\n",
    "random_beta = np.random.rand(X.shape[-1])\n",
    "\n",
    "custom_layer_norm.set_weights([random_alpha, random_beta])\n",
    "keras_layer_norm.set_weights([random_alpha, random_beta])\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23b9db-cdfc-4748-a682-12e3ca807a70",
   "metadata": {},
   "source": [
    "Still a negligeable difference! Our custom layer works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbbe9f-b4b9-4a70-8f71-a465b73237fb",
   "metadata": {},
   "source": [
    "## 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c322ef2c-6c05-4de1-9e84-a2c1fa62581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e6e1e1a9-de7f-49ca-bad8-fb3260b172e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "7e22fcd9-877b-4193-96e2-2c901b46150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianvi\\env\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "bfc5c7ee-d752-4a51-8281-f4b6e16aa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8503110b-a1b8-4097-92b6-14def4cd002d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e1341433234b719c7c7ab972b4a57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e565cce5c1649fcbd44eda5c1b06975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8dd5accdb74da78fdfa905302084eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a4da7bf7fc43a6a265839e034d6eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24732e69d744917bd9423403aa79376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c7396a857340249d82ae8286cd2b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "817af153-9eae-497c-a22a-bf1a3cdaee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "])\n",
    "upper_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model = tf.keras.Sequential([\n",
    "    lower_layers, upper_layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "928dc20e-44aa-4e00-ad32-4b1c71742804",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "upper_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f415567a-ee20-45a8-881b-431189d32d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "935e0e6b-d2de-440e-b2f7-610693988ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da4dea719846529f3f757cc4baa2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6d7e64fe5b4d4cbdd63e3890f0b2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae99d1c7e79e4f1d99715746276801c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f64c6fc1ed4b7195db46608f081e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a248f2ff6363474cb1aa859bbc52dcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5ca0a6eb174facaa8030576a9fa41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                for layers, optimizer in ((lower_layers, lower_optimizer),\n",
    "                                          (upper_layers, upper_optimizer)):\n",
    "                    gradients = tape.gradient(loss, layers.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "                del tape\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9df87-0a2c-4f65-84fe-560adc7401aa",
   "metadata": {},
   "source": [
    "`The error above happens because the output is not saved when using the library for pretty prints.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
